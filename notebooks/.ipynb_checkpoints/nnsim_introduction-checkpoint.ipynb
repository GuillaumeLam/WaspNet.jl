{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNsim Introduction\n",
    "This notebook serves as an introduction to `nnsim`, a Neural Network Simulation package for Julia. The goal of this package is to compactly represent neurons, layers of neurons, and finally networks comprising layers of neurons for the purposes of exploring the dynamics of neural networks. \n",
    "\n",
    "`nnsim` functions primarily as a framework, with neuron details intended to be implemented by the user. As of 12/19/19, `nnsim` is capable of simulating a network of neurons, but support for training/learning has not been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using nnsim\n",
    "using Parameters # Not necessary, but highly recommended for automating the generation of constructors for types\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now imported the general framework of `nnsim`. Let's begin by describing an Izhikevich neuron to build a network out of. We include the relevant parameters in the model, as well as the state, which must be an `array` for [reasons rooted in the Julia type system](https://stackoverflow.com/a/50163043/3630587). \n",
    "\n",
    "`@with_kw` is a macro which automatically creates a constructor with keyword arguments and default values for our `Izh` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Izh"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@with_kw struct Izh{F}<:AbstractNeuron # Our new type is a subtype of `AbtractNeuron`\n",
    "   a::F = 0.02      # a-d are model parameters\n",
    "   b::F = 0.2\n",
    "   c::F = -65.\n",
    "   d::F = 8.\n",
    "   I::F = 25.       # Background current injection (mA)\n",
    "   θ::F = 30.       # Threshold potential (mV)\n",
    "\n",
    "   v0::F = -65.     # Reset voltage (mV)\n",
    "   u0::F = 0.       # Reset state variable\n",
    "   state::Array{F,1} = [-65., 0.]      # Membrane potential (mV) and state variable\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two* other functions which are currently necessary to implement: `reset!` which does an in-place reset of the neuron's state to its initial value, and `update!` which processes an input value and evolves the state of the neuron in time.\n",
    "\n",
    "\\* In the future there will likely need to be a third function to implement which is used for learning/updates. For example, an ANN neuron needs to be able to compute gradients so that backpropagation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reset! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the given neuron with an input value `input_update` by evolving it via the Euler method a time \n",
    "#   duration `dt` with absolute time `t` (`t` is included in the case of parameters such as `I` potentially\n",
    "#   varying in time)\n",
    "function update!(neuron::Izh, input_update, dt, t)\n",
    "    retval = 0\n",
    "    # If an impulse came in, add it to the state\n",
    "    neuron.state[1] += input_update\n",
    "\n",
    "    # Euler method update\n",
    "    neuron.state .+= [\n",
    "        dt*(0.05 * neuron.state[1]^2 + 5*neuron.state[1] + 140 - neuron.state[2] + neuron.I),\n",
    "        dt*(neuron.a)*(neuron.b*neuron.state[1]-neuron.state[2])\n",
    "        ]\n",
    "\n",
    "    # Check for thresholding\n",
    "    if neuron.state[1] >= neuron.θ\n",
    "        neuron.state .= [ neuron.v0, neuron.state[2] + neuron.d]\n",
    "        retval = 1\n",
    "    end\n",
    "\n",
    "    return retval\n",
    "end\n",
    "\n",
    "# If reset the neuron state to its initial value\n",
    "function reset!(neuron::Izh)\n",
    "    neuron.state .= [neuron.v0, neuron.u0]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We had to describe the equation of motion for the neuron state and its update rules for spiking, and now we're able to construct a layer and network out of these neurons. To build a layer, we need to specify the neuron type, number of neurons, a weight matrix `W`, and any neuron parameters we'd like.\n",
    "\n",
    "We'll instantiate a 3-layer network of Izhikevich neurons with default parameters plus some noise on the parameters to introduce inhomogeneities in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "a1 = 0.02 .+ randn(N)/100\n",
    "b1 = 0.2 .+ randn(N)/10\n",
    "c1 = -65. .+ randn(N)\n",
    "d1 = 8. .+ randn(N)/10\n",
    "I1 = 25 .+ randn(N)\n",
    "\n",
    "W = randn(N,N)\n",
    "\n",
    "layer1 = Batch_Layer_Construction(Izh, W, N, a = a1, b = b1, c = c1, d = d1, I = I1);\n",
    "layer2 = Batch_Layer_Construction(Izh, W, N, a = a1, b = b1, c = c1, d = d1, I = I1);\n",
    "layer3 = Batch_Layer_Construction(Izh, W, N, a = a1, b = b1, c = c1, d = d1, I = I1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([layer1, layer2, layer3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching update!(::Izh{Float64}, ::Float64, ::Float64, ::Float64)\nClosest candidates are:\n  update!(!Matched::nnsim.LIF, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/neurons.jl:15\n  update!(!Matched::nnsim.Izh, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/neurons.jl:51\n  update!(!Matched::Layer, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/layer.jl:11\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching update!(::Izh{Float64}, ::Float64, ::Float64, ::Float64)\nClosest candidates are:\n  update!(!Matched::nnsim.LIF, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/neurons.jl:15\n  update!(!Matched::nnsim.Izh, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/neurons.jl:51\n  update!(!Matched::Layer, ::Any, ::Any, ::Any) at /home/buercklin/Documents/Projects/nnsim/src/layer.jl:11\n  ...",
      "",
      "Stacktrace:",
      " [1] _broadcast_getindex at ./broadcast.jl:578 [inlined]",
      " [2] getindex at ./broadcast.jl:511 [inlined]",
      " [3] copy at ./broadcast.jl:787 [inlined]",
      " [4] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1},Nothing,typeof(nnsim.update!),Tuple{Array{Izh{Float64},1},Array{Float64,1},Float64,Float64}}) at ./broadcast.jl:753",
      " [5] update!(::Layer{Izh{Float64}}, ::Array{Float64,1}, ::Float64, ::Float64) at /home/buercklin/Documents/Projects/nnsim/src/layer.jl:12",
      " [6] #13 at /home/buercklin/Documents/Projects/nnsim/src/network.jl:21 [inlined]",
      " [7] mapfoldl_impl(::typeof(identity), ::getfield(nnsim, Symbol(\"##13#14\")){Float64,Float64}, ::NamedTuple{(:init,),Tuple{Array{Float64,1}}}, ::Array{Layer{Izh{Float64}},1}) at ./reduce.jl:45",
      " [8] #mapfoldl#187(::Base.Iterators.Pairs{Symbol,Array{Float64,1},Tuple{Symbol},NamedTuple{(:init,),Tuple{Array{Float64,1}}}}, ::Function, ::Function, ::Function, ::Array{Layer{Izh{Float64}},1}) at ./reduce.jl:72",
      " [9] #mapfoldl at ./none:0 [inlined]",
      " [10] #foldl#188 at ./reduce.jl:90 [inlined]",
      " [11] (::getfield(Base, Symbol(\"#kw##foldl\")))(::NamedTuple{(:init,),Tuple{Array{Float64,1}}}, ::typeof(foldl), ::Function, ::Array{Layer{Izh{Float64}},1}) at ./none:0",
      " [12] update!(::Network, ::Array{Float64,1}, ::Float64, ::Float64) at /home/buercklin/Documents/Projects/nnsim/src/network.jl:20",
      " [13] simulate!(::Network, ::Array{Float64,1}, ::Float64, ::Int64) at /home/buercklin/Documents/Projects/nnsim/src/network.jl:37",
      " [14] top-level scope at In[11]:1"
     ]
    }
   ],
   "source": [
    "output = nnsim.simulate!(net, 2. * ones(N), .001, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
